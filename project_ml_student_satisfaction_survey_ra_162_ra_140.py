# -*- coding: utf-8 -*-
"""Project_ML_Student_Satisfaction_Survey_RA_162_RA_140.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1DFuw9-MoAsG0XfAFSSSto7cQnXR7bDnG
"""

#Importing the libraries

import pandas as pd  #Data manipulation
import numpy as np   #Data manipulation
import matplotlib.pyplot as plt  # Visualization
import seaborn as sb  # Visualization
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn import preprocessing
from sklearn.metrics import mean_squared_error,mean_absolute_error,accuracy_score

#Loading the dataset collected from the google form into an excel sheet
df=pd.read_csv('/content/SRM Student Academic Satisfaction Survey (Responses).csv')

#Lets look into top few rows and columns in the dataset
df.head(10)

#Pre-Processing of the data
#1) To check whether there is missing values or not in the data
df.isnull().sum()

#2) Removing redundant features by keeping the numeric data only
df2=df.select_dtypes(include=[np.number])
df2.head(5)

#3) Checking for Outlier if any
for i in df2.columns:
    sb.boxplot(x=df2[i])
    plt.show()

#4)Outlier removal by dropping the outlier
for i in df2.columns: 
     q1=df2[i].quantile(0.25)
     q3=df2[i].quantile(0.75)
     iqr=q3-q1
     ub=q3+1.5*iqr
     lb=q1-1.5*iqr
     df3=df2[((df2[i]<ub) & (df2[i]>lb))]
     sb.boxplot(x=df3[i])
     plt.show()

#Training the Model

#Splitting the data into training and testing data

x_train,x_test,y_train,y_test=train_test_split(df3.drop('Rate Overall Satisfaction',axis=1),df3['Rate Overall Satisfaction'],test_size=0.20,random_state=42)

#
regressor = LinearRegression()
regressor.fit(x_train, y_train)

#
y_pred = regressor.predict(x_test)

#Plotting the predicted values with actual values

fig,ax=plt.subplots()
sb.regplot(x=y_test,y=y_pred,ci=None,scatter_kws={"color": "green"}, line_kws={"color": "red"});
scatter=ax.plot([],[],'go',label='y_test')
line=ax.plot([],[],'r',label='y_pred')
plt.legend()
plt.show()

#Accuracy -  R2 Score is between 0 and 1, the closer to 1, the better the regression fit.
#From the below given calculation we get the regression fit is good as it is closer to 1.

print("The accuracy of the following linear regression is : ",regressor.score(x_test,y_test))

#Accuracy on the basis of percentage
from sklearn.metrics import r2_score
score = r2_score(y_test, y_pred)
print("The accuracy of our model is {}%".format(round(score, 2) *100))

# To compare the linear regression accuracy we use the decision tree model
from sklearn import svm
SVM=svm.SVC()
SVM.fit(x_train,y_train)
predictsvm=SVM.predict(x_test)
accuracy=accuracy_score(y_test,predictsvm)
print(accuracy)
print("This shows that the linear regression has good accuracy when compared with the SVM model which calculated the accuracy of the same dataset.")

# measuring the RMSE
rmse = np.sqrt(mean_squared_error(y_test,y_pred))
print("Root Mean Squared Error of the model : ", rmse)



